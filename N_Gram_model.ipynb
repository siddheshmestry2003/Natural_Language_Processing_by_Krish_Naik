{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a83558f7-c968-49a5-a10d-059a5c62be1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4  spam  Congratulations! You've won a $1000 Walmart gi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopword: Package 'stopword' not found in\n",
      "[nltk_data]     index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "message=pd.read_csv(\"sms_spam_collection.csv\")\n",
    "print(message)\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopword')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e53b6e44-ec58-4e10-87fa-e154c976d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6229ddd8-ebbe-4f66-9557-d626a5896e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go jurong point crazi avail bugi n great world la e buffet', 'ok lar joke wif u oni', 'free entri wkli comp win fa cup final tkt st may', 'u dun say earli hor u c alreadi say', 'congratul walmart gift card call']\n"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "for i in range(0,len(message)):\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \", message['message'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b668364-8f4d-400f-8fc2-50565216895c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "        1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create output variable\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv1=CountVectorizer(max_features=25,binary=True,)\n",
    "\n",
    "x=cv1.fit_transform(corpus).toarray()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8038e88-ca43-47c2-b9a2-b7c525b08894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'go': np.int64(17),\n",
       " 'jurong': np.int64(21),\n",
       " 'crazi': np.int64(8),\n",
       " 'avail': np.int64(1),\n",
       " 'bugi': np.int64(3),\n",
       " 'great': np.int64(18),\n",
       " 'la': np.int64(22),\n",
       " 'buffet': np.int64(2),\n",
       " 'lar': np.int64(23),\n",
       " 'joke': np.int64(20),\n",
       " 'free': np.int64(15),\n",
       " 'entri': np.int64(12),\n",
       " 'comp': np.int64(6),\n",
       " 'fa': np.int64(13),\n",
       " 'cup': np.int64(9),\n",
       " 'final': np.int64(14),\n",
       " 'may': np.int64(24),\n",
       " 'dun': np.int64(10),\n",
       " 'earli': np.int64(11),\n",
       " 'hor': np.int64(19),\n",
       " 'alreadi': np.int64(0),\n",
       " 'congratul': np.int64(7),\n",
       " 'gift': np.int64(16),\n",
       " 'card': np.int64(5),\n",
       " 'call': np.int64(4)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8833966-235d-44a5-9240-5ac9ac40ddcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create output variable\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv2=CountVectorizer(max_features=25,binary=True,ngram_range=(2,3))  #ngram_range(1,2) inside backet is combination of grams that are unigram and bigram\n",
    "\n",
    "x1=cv2.fit_transform(corpus).toarray()\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77ba5ae6-d5af-48af-ba26-1eed220917b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'go jurong': np.int64(15),\n",
       " 'jurong point': np.int64(19),\n",
       " 'point crazi': np.int64(23),\n",
       " 'crazi avail': np.int64(6),\n",
       " 'avail bugi': np.int64(1),\n",
       " 'bugi great': np.int64(2),\n",
       " 'great world': np.int64(16),\n",
       " 'la buffet': np.int64(20),\n",
       " 'ok lar': np.int64(22),\n",
       " 'lar joke': np.int64(21),\n",
       " 'joke wif': np.int64(18),\n",
       " 'free entri': np.int64(13),\n",
       " 'entri wkli': np.int64(10),\n",
       " 'comp win': np.int64(4),\n",
       " 'fa cup': np.int64(11),\n",
       " 'cup final': np.int64(7),\n",
       " 'final tkt': np.int64(12),\n",
       " 'dun say': np.int64(8),\n",
       " 'say earli': np.int64(24),\n",
       " 'earli hor': np.int64(9),\n",
       " 'hor alreadi': np.int64(17),\n",
       " 'alreadi say': np.int64(0),\n",
       " 'congratul walmart': np.int64(5),\n",
       " 'gift card': np.int64(14),\n",
       " 'card call': np.int64(3)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62dd70-2308-45c7-b5af-f0673bccbfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
